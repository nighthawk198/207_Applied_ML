{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Sentence Classification with LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nighthawk198/207_Applied_ML/blob/master/Sentence_Classification_with_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4skkbKsOaP1w",
        "colab_type": "text"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6R1blmZaP1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from unicodedata import normalize\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os, re\n",
        "import psutil\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# keras module for building LSTM \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import keras.utils as ku \n",
        "\n",
        "# for pre-trained embeddings\n",
        "import gensim\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models.phrases import Phraser, Phrases\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ponos17kaP12",
        "colab_type": "text"
      },
      "source": [
        "## Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNrz18vJke2i",
        "colab_type": "code",
        "outputId": "b279c4a9-755c-4b53-ed84-15c42fc5f9d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fmFBeR6qYoJ",
        "colab_type": "code",
        "outputId": "433d843b-6ce3-414c-925b-9ca93a32fb87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "final_file = open(os.path.join(\"drive/My Drive\", \"data/reddit_train_test_capped.pkl\"),'rb')\n",
        "train_df, test_df= pickle.load(final_file),  pickle.load(final_file)\n",
        "\n",
        "final_file.close()\n",
        "train_df.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GoLr6ZnrJXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# subset to only the fields I will need\n",
        "train_df = train_df[['score','body','is_popular']]\n",
        "test_df = test_df[['score','body','is_popular']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNkAV4Jc__wG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove nan in body, the input\n",
        "train_df.dropna(subset=['body'], inplace=True)\n",
        "test_df.dropna(subset=['body'], inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd1b-uurgAfp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Google News embeddings based on 3M words in 300 dimensions\n",
        "filename = os.path.join(\"drive/My Drive\", \"data/GoogleNews-vectors-negative300.bin\")\n",
        "gensim_embeddings = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
        "\n",
        "pretrained_weights = gensim_embeddings.wv.syn0\n",
        "vocab_size, embedding_size = pretrained_weights.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e4lFSEgWhijB",
        "colab": {}
      },
      "source": [
        "df_data = pd.read_csv(os.path.join(\"drive/My Drive\", \"data/all_tweets_clean_v2.csv\"))\n",
        "\n",
        "_, t_test_df = train_test_split(df_data, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoPQQFZKaP16",
        "colab_type": "text"
      },
      "source": [
        "## Dataset Prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GtgNelyaP16",
        "colab_type": "text"
      },
      "source": [
        "### Dataset Cleaning\n",
        "\n",
        "[My notes]: The data is relatively clean as-is, so we don't have to do that much cleaning. If reddit, might need to do more. If Twitter, think it'll be relatively easy since the data is somewhat clean as is. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fog-rdhXaP17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(txt):\n",
        "  txt = re.sub(r'https:\\/\\/t[.]co\\/[A-Za-z0-9]*$', '', txt)\n",
        "  txt = re.sub(r'\\n', ' ', txt)\n",
        "  txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
        "  txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
        "  txt = re.sub(' +', ' ', txt)\n",
        "  return(txt)\n",
        "\n",
        "train_corpus = train_df['body'].apply(clean_text)\n",
        "test_corpus = test_df['body'].apply(clean_text)\n",
        "t_test_corpus = t_test_df['body'].apply(clean_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr9NRbkLaP19",
        "colab_type": "text"
      },
      "source": [
        "### Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7qcF-7AaP1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "# tokenize our text\n",
        "tokenizer.fit_on_texts(train_corpus)\n",
        "# turn text into token sequence\n",
        "train_sequences = tokenizer.texts_to_sequences(train_corpus)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_corpus)\n",
        "t_test_sequences = tokenizer.texts_to_sequences(t_test_corpus)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBDepPwTaP2B",
        "colab_type": "text"
      },
      "source": [
        "### Padding Sequences and Obtaining Variables: Predictors and Targets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz2KRq4g7_Pd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b6b16a4a-0c36-481a-b47d-d3ee47990043"
      },
      "source": [
        "x_train = pad_sequences(train_sequences, maxlen = 100)\n",
        "x_test = pad_sequences(test_sequences, maxlen = 100)\n",
        "t_x_test = pad_sequences(t_test_sequences, maxlen = 100)\n",
        "print(t_x_test.shape)\n",
        "\n",
        "y_train = train_df['is_popular'].tolist()\n",
        "y_test = test_df['is_popular'].tolist()\n",
        "t_y_test = t_test_df['is_popular'].tolist()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3339, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQX2bLJfaP2E",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjcxNnuoaP2E",
        "colab_type": "text"
      },
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYkBYebBMKXk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "23abb8cf-ba3c-4669-c454-f08352d1bcbf"
      },
      "source": [
        "def create_model2(embedding_vectors):\n",
        "  model = Sequential()\n",
        "  \n",
        "  \n",
        "  model.add(Embedding(input_dim=vocab_size,\n",
        "                      output_dim=embedding_size,\n",
        "                      weights=[pretrained_weights],\n",
        "                      trainable=False,\n",
        "                      name='embedding_layer'))\n",
        "  \n",
        "  model.add(LSTM(100))\n",
        "  \n",
        "  #model.add(Dropout(0.1))\n",
        "  \n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  \n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  \n",
        "  return(model)\n",
        "\n",
        "model = create_model2(embedding_vectors=100)\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0803 07:05:50.754146 139793014032256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0803 07:05:50.771279 139793014032256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0803 07:05:50.774182 139793014032256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0803 07:05:50.783721 139793014032256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0803 07:05:50.784650 139793014032256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0803 07:05:57.815107 139793014032256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0803 07:05:57.835648 139793014032256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_layer (Embedding)  (None, None, 300)         900000000 \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               160400    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 900,160,501\n",
            "Trainable params: 160,501\n",
            "Non-trainable params: 900,000,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl0KKgx3aP2H",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFeENlPJN5bi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "6bb6f19f-10ec-4c5f-c5c2-9bef81514ced"
      },
      "source": [
        "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=64)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 49846 samples, validate on 14949 samples\n",
            "Epoch 1/3\n",
            "49846/49846 [==============================] - 113s 2ms/step - loss: 0.2756 - acc: 0.9226 - val_loss: 0.2738 - val_acc: 0.9221\n",
            "Epoch 2/3\n",
            "49846/49846 [==============================] - 111s 2ms/step - loss: 0.2705 - acc: 0.9229 - val_loss: 0.2745 - val_acc: 0.9221\n",
            "Epoch 3/3\n",
            "49846/49846 [==============================] - 110s 2ms/step - loss: 0.2695 - acc: 0.9229 - val_loss: 0.2743 - val_acc: 0.9221\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f237846ec18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SFaAYGbVVFJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8bead068-d2d0-4605-b582-a9c1c871171b"
      },
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Accuracy:{}\".format(scores[1]*100))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:92.2068365776975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93nUEX16kpSd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12fc46cf-848e-42ef-8760-359f6ca7bc28"
      },
      "source": [
        "model.predict(x_test).shape"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14949, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AvJhceNWkKe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "aedc9588-b08c-426c-9079-38b861899441"
      },
      "source": [
        "reddit_test_df=os.path.join(\"drive/My Drive\", \"data/reddit_train_test_capped.pkl\")\n",
        "twitter_test_df=os.path.join(\"drive/My Drive\", \"data/twitter_train_test_smaller_v2.pkl\")\n",
        "  \n",
        "final_file = open(reddit_test_df,'rb')\n",
        "_ , reddit_test_df =  pickle.load(final_file),  pickle.load(final_file)\n",
        "final_file.close()\n",
        "  \n",
        "final_file = open(twitter_test_df,'rb')\n",
        "_ , twitter_test_df =  pickle.load(final_file),  pickle.load(final_file)\n",
        "final_file.close()\n",
        "  \n",
        "mod = \"lstm_sentence_classification\"\n",
        "\n",
        "reddit_test_df.dropna(subset=['body'], inplace=True)\n",
        "\n",
        "reddit_test_df[mod] = model.predict(x_test)\n",
        "print(\"twitter_test_df\", twitter_test_df.shape)\n",
        "print(\"t_x_test\", t_x_test.shape)\n",
        "twitter_test_df[mod] = model.predict(t_x_test)\n",
        "    \n",
        "reddit_test_df.to_csv('drive/My Drive/models/reddit_test_predictions_mj.csv',index=False)\n",
        "twitter_test_df.to_csv('drive/My Drive/models/twitter_test_predictions_mj.csv',index=False)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "twitter_test_df (3339, 7)\n",
            "t_x_test (3339, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}